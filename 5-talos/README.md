## Talos Configuration

This documentation includes that 2 node(1 master, 1 worker) cluster configuration using Talos on UTM virtualization tool.

### Warning:
> I did not know what Talos is. Although I had not heard ever about it, I spent a few days to learn Talos and I liked it.

The resources I used to learn Talos are except the offical documentation:
1. https://www.youtube.com/watch?v=YdQCeU7NOak ( for installation )
2. https://www.youtube.com/watch?v=9CIMTum9bTA ( for basics )
3. https://www.youtube.com/watch?v=iEFb2Zg4xUg&t=200s (for overview and extra information)


## Usage
The commands below will setup the cluster and apply the manifests to your machines but you should customize the manifests to your needs.
```bash
git clone https://github.com/saidimtaylan/sample-task-devops.git
cd sample-task-devops/5-talos
bash setup.sh
```

## Notes
- I used the default master and worker config that was generated by Talos.
- I edited the machine.install.disk from /dev/sda to /dev/vda in controlplane and worker.yaml files because the virtualizer mounts the disk to /dev/vda. I counter some troubles because of this.
- I added the nameservers to the machine.network.nameservers in controlplane.yaml and worker.yaml files because the virtualizer does not have a default nameserver. So, Talos could not resolve the DNS and couldn't download the components.

## Managing the Cluster
- Check the cluster general and services health.
```bash
talosctl health
talosctl services
```
- View the all cluster members/nodes.
```bash
talosctl get members
```
- Monitor the whole cluster includes cpu, memory, disk, network..., components etc.
```bash
talosctl monitor
```
- View the node or whole cluster standard buffer outputs.
```bash
talosctl dmesg [-n 192.168.64.36]
```
- View component logs of of any node.
```bash
talosctl logs -f kubelet [-n 192.168.64.36]
```
- Upgrade the talos itself in a machine.
```bash
talosctl upgrade --nodes 192.168.64.36 --image ghcr.io/siderolabs/installer:v1.9.1
```
- Upgrade the kubernetes node in talos.
```bash
talosctl --nodes 192.168.64.36 upgrade-k8s --to 1.32.0 --dry-run # preview the changes
talosctl --nodes 192.168.64.36 upgrade-k8s --to 1.32.0 # apply the changes  
```
- Add new node to the cluster.
```bash
talosctl apply-config --nodes 192.168.64.36 --file talos-manifests/worker.yaml
```

### Troubleshooting
> I have not faced any problems yet because I am new to Talos. I would use the similar path which I use in my current company as solve the problems.
1. If some applications do not work, I would apply the same procedure which I used in the kubernetes directory to troubleshoot the problem (to go from pod to ingress one by one as sen request to ip addresses).
2. If the problem is not in the K8S cluster itself and I cannot connect to cluster via `talosctl` and a problem accoured all of a sudden, probably the problem is in the infrastructure itself. I would check the vms is running, firewall rules whether change or not, and the network connection, cpu, ram, disk usage of the nodes thorugh `talos health` and `talosctl dashboard`.
3. If there is not a problem in the infrastructure, I would check the logs of the nodes and components.
4. If I still could not find the problem, I would try to find the problem by tearing down the cluster main tools one by one like argocd, metallb, ingress controller...

### Deploy a Workload
#### Deploy the required dependencies
```bash
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm repo add argo https://argoproj.github.io/argo-helm # optional
helm repo update
helm install ingress-nginx ingress-nginx/ingress-nginx
helm install argocd argo/argo-cd # optional
kubectl edit service/ingress-nginx-controller -n ingress-nginx # We need to change the type of the service from LoadBalancer to NodePort since we have not load balancer and a public IP address.
```
#### Deploy the workload
```bash
kubectl create namespace production
kubectl kustomize k8s-application/overlays/production/ | kubectl apply -f -
```

#### Check the application
```bash
sudo echo "192.168.64.36 devops.dev" >> /etc/hosts # add the ip address to the hosts file to test the application
kubectl get svc service/ingress-nginx-controller -n ingress-nginx # get the nodeport of the ingress-nginx-controller
curl devops.dev:<nodeport> # test the application
```